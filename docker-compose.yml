version: '3.8'

services:
  # Main Cyber Assessment Reviewer application
  cyber-assessment-reviewer:
    build:
      context: .
      dockerfile: Dockerfile
      target: production  # Use core dependencies by default
    container_name: cyber-assessment-reviewer
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      # Application Configuration
      - FLASK_ENV=production
      - DEBUG=false
      - SECRET_KEY=${SECRET_KEY:-cyber-assessment-secret-change-in-production}
      - USE_PRODUCTION_SERVER=true
      - HOST=0.0.0.0
      - PORT=5000
      
      # AI Backend Configuration
      - USE_OLLAMA=true
      - OLLAMA_BASE_URL=http://ollama:11434
      - DEFAULT_MODEL_NAME=mistral:7b-instruct
      
      # WSGI Server Configuration
      - WSGI_WORKERS=4
      - WSGI_THREADS=4
      - WSGI_TIMEOUT=120
      
      # Logging Configuration
      - LOG_LEVEL=INFO
      - LOG_FILE=/app/logs/cyber_assessment_reviewer.log
      
      # File Upload Configuration
      - MAX_CONTENT_LENGTH=52428800  # 50MB
      
      # Skip dependency check in container
      - SKIP_DEP_CHECK=true
    volumes:
      # Persistent data volumes
      - ./data/uploads:/app/uploads
      - ./data/sessions:/app/sessions
      - ./data/logs:/app/logs
      - ./data/models:/app/models
    networks:
      - cyber-assessment-network
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/system_status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Ollama service for AI model serving
  ollama:
    image: ollama/ollama:latest
    container_name: cyber-assessment-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      # Persistent model storage
      - ./data/ollama:/root/.ollama
    networks:
      - cyber-assessment-network
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Model initialization service (runs once to pull models)
  ollama-init:
    image: ollama/ollama:latest
    container_name: cyber-assessment-ollama-init
    depends_on:
      - ollama
    networks:
      - cyber-assessment-network
    environment:
      - OLLAMA_HOST=http://ollama:11434
    command: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        until curl -f http://ollama:11434/api/tags; do
          echo 'Waiting for Ollama...'
          sleep 5
        done &&
        echo 'Ollama is ready. Pulling models...' &&
        ollama pull mistral:7b-instruct &&
        echo 'Models pulled successfully!'
      "
    restart: "no"  # Run only once

networks:
  cyber-assessment-network:
    driver: bridge
    name: cyber-assessment-network

volumes:
  # Named volumes for better management
  uploads:
    driver: local
  sessions:
    driver: local
  logs:
    driver: local
  models:
    driver: local
  ollama:
    driver: local
